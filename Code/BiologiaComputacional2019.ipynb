{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiologiaComputacional2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuDpR8wDgL9l",
        "colab_type": "text"
      },
      "source": [
        "#Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ljxGUbcgOTE",
        "colab_type": "code",
        "outputId": "c177d374-7d56-4592-8c9d-8bb595b8f725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "\n",
        "# IMPORTS REFERENTES AS BIBLIOTECAS USADAS NO PROJETO\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.optimizers import SGD\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "import time\n",
        "from keras.models import model_from_json\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn import preprocessing\n",
        "from scipy.stats import friedmanchisquare\n",
        "!pip install -q SwarmPackagepy\n",
        "!git clone https://github.com/IgorSouza21/SecundaryStructurePrediction.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for SwarmPackagepy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'SecundaryStructurePrediction'...\n",
            "remote: Enumerating objects: 525, done.\u001b[K\n",
            "remote: Counting objects: 100% (525/525), done.\u001b[K\n",
            "remote: Compressing objects: 100% (522/522), done.\u001b[K\n",
            "remote: Total 525 (delta 3), reused 521 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (525/525), 1.88 MiB | 3.44 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG-JyilzDp1h",
        "colab_type": "text"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raDFhRQIDrEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ETAPA REFERENTE AO PRE PROCESSAMENTO E CONSTRUÇÃO DA BASE\n",
        "\n",
        "import glob\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import keras\n",
        "\n",
        "def conversion8to3(value):\n",
        "    #     helix (G, H and I), strand (E and B) and loop (S, T, and _)\n",
        "    c = {\"H\": [\"G\", \"H\", \"I\"], \"S\": [\"E\", \"B\"], \"_\": [\"S\", \"T\", \"_\", \"?\"]}\n",
        "    for t in c.items():\n",
        "        if value in t[1]:\n",
        "            return t[0]\n",
        "\n",
        "\n",
        "def qualquercoisa(v):\n",
        "    if v == \"?\":\n",
        "        return \"_\"\n",
        "    else:\n",
        "        return v\n",
        "        \n",
        "\n",
        "def loadfolderproteins(path):\n",
        "    base = pd.DataFrame()\n",
        "    res = []\n",
        "    dssp8 = []\n",
        "    dssp3 = []\n",
        "    for i in glob.glob(path):\n",
        "        f = open(i, \"r\")\n",
        "        x = f.readlines()\n",
        "        f.close()\n",
        "        prot = x[0].split(\"RES:\")[1]\n",
        "        prot = prot.split(\",\")\n",
        "        prot.pop()\n",
        "        res.append(\"\".join(prot))\n",
        "        lb = x[1].split(\"DSSP:\")[1]\n",
        "        lb = lb.split(\",\")\n",
        "        lb.pop()\n",
        "        lba = [qualquercoisa(bl) for bl in lb]\n",
        "        lb3 = [conversion8to3(bl) for bl in lb]\n",
        "        dssp8.append(\"\".join(lba))\n",
        "        dssp3.append(\"\".join(lb3))\n",
        "\n",
        "    base[\"RES\"] = res\n",
        "    base['DSSP8'] = dssp8\n",
        "    base['DSSP3'] = dssp3\n",
        "    base.to_csv(\"SecundaryStructurePrediction/NewCB513.csv\", index=False)\n",
        "\n",
        "\n",
        "def load(X, size, window, classe=\"DSSP3\"):\n",
        "    data = X['RES']\n",
        "    classes = X[classe]\n",
        "    positions = {'A':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'K':9,'L':10,'M':11,'N':12,\n",
        "                 'P':13,'Q':14,'R':15,'S':16,'T':17,'V':18,'W':19,'Y':20, 'X': 21, 'B': 22, 'Z': 23}\n",
        "    classes_conv = {'S': 0, 'I': 1, 'G': 2, 'E': 3, 'T': 4, 'H': 5, 'B': 6, '_': 7}\n",
        "    res = []\n",
        "    for i in range(len(data)):\n",
        "        for j in range(len(data[i])):\n",
        "            res.append(int(positions[data[i][j]]))\n",
        "    for i in range(size):\n",
        "        res.insert(0, 0)\n",
        "        res.append(0)\n",
        "    data =[]\n",
        "    for i in range(len(X['RES'])):\n",
        "        data.append(res[i:window + i])\n",
        "#         data[i].append(classes[i])\n",
        "        data[i].append(classes_conv[classes[i]])\n",
        "    columns = [[]]*(window + 1)\n",
        "    for i in range(window):\n",
        "        columns[i] = \"Element\" + str(i)\n",
        "    columns[window] = 'Class'\n",
        "    data = pd.DataFrame(data, columns = columns)\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "def load_data(window):\n",
        "    size = int(window/2)\n",
        "    proteins = pd.read_csv('SecundaryStructurePrediction/NewCB513.csv')\n",
        "    res = load(proteins.iloc[0], size, window)\n",
        "    for i in range(1,len(proteins)):\n",
        "        aux = load(proteins.iloc[i], size, window,classe='DSSP3')\n",
        "        res = res.append(aux,ignore_index = True)\n",
        "        \n",
        "    X = res.drop(['Class'], axis=1)\n",
        "    y = res['Class']\n",
        "    y = np.array(y)\n",
        "    for x in range(len(y)):\n",
        "        if y[x] == 5:\n",
        "            y[x] = 1\n",
        "        elif y[x] == 7:\n",
        "            y[x] = 2\n",
        "        \n",
        "    return X,y#pd.DataFrame(keras.utils.to_categorical(y))\n",
        "\n",
        "loadfolderproteins(\"SecundaryStructurePrediction/513_distribute/*.all\")\n",
        "#dataset, labels = load_data(15)\n",
        "\n",
        "      \n",
        "#print(labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOzrbTmLpGPW",
        "colab_type": "text"
      },
      "source": [
        "#Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyCbvAvzpHba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FUNÇÕES UTILIZADAS NO PROJETO \n",
        "\n",
        "def amostra_estrat(tam , df , classe):\n",
        "    classes = df[classe].unique()\n",
        "    qtde_por_classe = round(tam / len(classes))\n",
        "    amostras_por_classe = []\n",
        "    for c in classes:\n",
        "        indices_c = df[classe] == c\n",
        "        obs_c = df[indices_c]\n",
        "        amostra_c = obs_c.sample(qtde_por_classe)\n",
        "        amostras_por_classe.append(amostra_c)\n",
        "    amostra_estratificada = pd.concat(amostras_por_classe)\n",
        "    return amostra_estratificada\n",
        "  \n",
        "windowsize = 5\n",
        "arrayfitnessGWO = []\n",
        "\n",
        "model = Sequential() # Instanciando o modelo\n",
        "model.add(Dense(120, input_dim = windowsize, activation=\"relu\",name='dense1'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(60,activation='relu',name='dense2'))\n",
        "model.add(Dense(3, activation=\"softmax\",name='dense3')) # Instanciando o modelo\n",
        "model.compile( loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy'])\n",
        "\n",
        "def normalize(df1):\n",
        "    x = df1.values.astype(float)\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    scaled = min_max_scaler.fit_transform(x)\n",
        "    df_normalized = pd.DataFrame(scaled)\n",
        "    return df_normalized \n",
        "  \n",
        "def creatpopuMSRA(sizepopu,sizecromo):    \n",
        "    limit = np.sqrt(6/float(sizecromo))\n",
        "    chromossomos = [np.random.uniform(low = -limit,high = limit, size = sizecromo) for x in range(sizepopu)]\n",
        "    return chromossomos\n",
        "\n",
        "def fitnessGWO(chromossome,Xtrain,ytrain,Xval,yval):\n",
        "    \n",
        "    w1 = np.reshape(np.split(np.array(chromossome[0:600]),120),(5,120))           #np.split(np.array(chromossome[0:len(chromossome)-6]), 50)\n",
        "    w2 = np.reshape(np.split(np.array(chromossome[600:7800]),60),(120,60))           #np.reshape(chromossome[len(chromossome)-7:len(chromossome)-1], (6, 1))\n",
        "    w3 = np.reshape(chromossome[7800:7980], (60,3))\n",
        "    \n",
        "    model.get_layer('dense1').set_weights((np.array(w1), np.zeros(120)))\n",
        "    model.get_layer('dense2').set_weights((np.array(w2), np.zeros(60)))\n",
        "    model.get_layer('dense3').set_weights((np.array(w3), np.zeros(3)))\n",
        "    model.fit(Xtrain, ytrain, epochs=100, batch_size=512, verbose = False)\n",
        "    retorno = model.evaluate(Xval,yval, verbose = False)\n",
        "\n",
        "    if len(arrayfitnessGWO) == 0:\n",
        "        model_json = model.to_json()\n",
        "        with open(\"modelGWO.json\", \"w\") as json_file:\n",
        "            json_file.write(model_json)\n",
        "        model.save_weights(\"modelGWO.h5\")\n",
        "\n",
        "    atual = retorno[1]\n",
        "\n",
        "    if len(arrayfitnessGWO) > 1:\n",
        "        if atual > arrayfitnessGWO[np.argmax(arrayfitnessGWO)]:\n",
        "            model_json = model.to_json()\n",
        "            with open(\"modelGWO.json\", \"w\") as json_file:\n",
        "                json_file.write(model_json)\n",
        "            model.save_weights(\"modelGWO.h5\")\n",
        "\n",
        "    arrayfitnessGWO.append(atual)\n",
        "\n",
        "    return atual\n",
        "\n",
        "def loadmodelGWO():\n",
        "    json_file = open('modelGWO.json', 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model = model_from_json(loaded_model_json)\n",
        "    loaded_model.load_weights(\"modelGWO.h5\")\n",
        "    loaded_model.compile( loss='binary_crossentropy', optimizer=\"adam\", \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "    return loaded_model\n",
        "\n",
        "\n",
        "from SwarmPackagePy.intelligence import sw\n",
        "\n",
        "\n",
        "class gwo(sw):\n",
        "\n",
        "    def __init__(self, n, function, lb, ub, dimension, iteration,Xtrain,ytrain,Xval,yval,init = None):\n",
        "\n",
        "\n",
        "        super(gwo, self).__init__()\n",
        "\n",
        "        if init is None:\n",
        "            self.__agents = np.random.uniform(lb, ub, (n, dimension))\n",
        "        else:\n",
        "            self.__agents = np.array(init)\n",
        "            \n",
        "        self._points(self.__agents)\n",
        "        alpha, beta, delta = self.__get_abd(n, function,Xtrain,ytrain,Xval,yval)\n",
        "\n",
        "        Gbest = alpha\n",
        "\n",
        "        for t in range(iteration):\n",
        "\n",
        "            a = 2 - 2 * t / iteration\n",
        "\n",
        "            r1 = np.random.random((n, dimension))\n",
        "            r2 = np.random.random((n, dimension))\n",
        "            A1 = 2 * r1 * a - a\n",
        "            C1 = 2 * r2\n",
        "\n",
        "            r1 = np.random.random((n, dimension))\n",
        "            r2 = np.random.random((n, dimension))\n",
        "            A2 = 2 * r1 * a - a\n",
        "            C2 = 2 * r2\n",
        "\n",
        "            r1 = np.random.random((n, dimension))\n",
        "            r2 = np.random.random((n, dimension))\n",
        "            A3 = 2 * r1 * a - a\n",
        "            C3 = 2 * r2\n",
        "\n",
        "            Dalpha = abs(C1 * alpha - self.__agents)\n",
        "            Dbeta = abs(C2 * beta - self.__agents)\n",
        "            Ddelta = abs(C3 * delta - self.__agents)\n",
        "\n",
        "            X1 = alpha - A1 * Dalpha\n",
        "            X2 = beta - A2 * Dbeta\n",
        "            X3 = delta - A3 * Ddelta\n",
        "            self.__agents = (X1 + X2 + X3) / 3\n",
        "\n",
        "            self.__agents = np.clip(self.__agents, lb, ub)\n",
        "            self._points(self.__agents)\n",
        "\n",
        "            alpha, beta, delta = self.__get_abd(n, function,Xtrain,ytrain,Xval,yval)\n",
        "            if function(alpha,Xtrain,ytrain,Xval,yval) > function(Gbest,Xtrain,ytrain,Xval,yval):\n",
        "                Gbest = alpha\n",
        "\n",
        "        self._set_Gbest(Gbest)\n",
        "        alpha, beta, delta = self.__get_abd(n, function,Xtrain,ytrain,Xval,yval)\n",
        "        self.__leaders = list(alpha), list(beta), list(delta)\n",
        "\n",
        "    def __get_abd(self, n, function,Xtrain,ytrain,Xval,yval):\n",
        "\n",
        "        result = []\n",
        "        fitness = [(function(self.__agents[i],Xtrain,ytrain,Xval,yval), i) for i in range(n)]\n",
        "        fitness.sort(reverse=True)\n",
        "\n",
        "        for i in range(3):\n",
        "            result.append(self.__agents[fitness[i][1]])\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_leaders(self):\n",
        "\n",
        "        return list(self.__leaders)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-KvJBoSA5NR",
        "colab_type": "text"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-KLQ-3KA6wZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAIN DO PROJETO ONDE É REALIZADO OS TESTES\n",
        "\n",
        "ite = 2    # Quantidade de gerações dos algoritmos evolutivos \n",
        "iteKfold = 1 # Quantidade de iterações que vai ser executado o cross validation de k = 10\n",
        "scoresfinalMLP = [] # Array para salvar os score de validação da MLP\n",
        "cont = 0 # Seed para o shuffle da base de dados\n",
        "popu = 3 # Tamanho da população\n",
        "kvalue = 10\n",
        "\n",
        "dataset,labels = load_data(windowsize) # CARREGANDO A BASE\n",
        "dataset = normalize(dataset) # NORMALIZANDO A BASE\n",
        "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=False,save_best_only=True) # UTILIZANDO CHECKPOINTER PRA SALVAR CONFIGURAÇÃO DA MLP DURANTE AS EPOCAS\n",
        "\n",
        "testGWO = []\n",
        "testMLP = []\n",
        "\n",
        "k = StratifiedKFold(kvalue,True,1)\n",
        "# k = KFold(kvalue,True,1) # Instancia da classe Kfold com k = 10\n",
        "\n",
        "for x in range(iteKfold): # For da quantidade de vezes que ira ser executado o cross validation de k = 10\n",
        "    \n",
        "    c = k.split(dataset,labels)\n",
        "    labels = keras.utils.to_categorical(labels)\n",
        "    for train_index,test_index in c: # For do Kfold com K = 10\n",
        "\n",
        "        X_train, X_test = np.array(dataset)[train_index], np.array(dataset)[test_index] # Separando o conjunto de Treino separado a cima em treino e validação\n",
        "        y_train, y_test = np.array(labels)[train_index], np.array(labels)[test_index] # Separando o conjunto de Treino separado a cima em treino e validação\n",
        "\n",
        "        X_trainK, X_valK, y_trainK, y_valK = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "\n",
        "        ############ CREATE POPU ######\n",
        "\n",
        "        inicialpopulation = np.array(creatpopuMSRA(popu,7980)) # Criando a população inicial\n",
        "        sizepop = inicialpopulation.shape[0] # Tamanho da população\n",
        "        dimpop = inicialpopulation.shape[1] # Dimensão da população\n",
        "\n",
        "        ############ CREATE POPU #########\n",
        "\n",
        "        ######## MLP ############\n",
        "\n",
        "        modelMLP = Sequential() # Instanciando o modelo\n",
        "        modelMLP.add(Dense(120, input_dim = windowsize, activation=\"relu\"))\n",
        "        modelMLP.add(Dropout(0.3))\n",
        "        modelMLP.add(Dense(60,activation='relu'))\n",
        "        modelMLP.add(Dense(3, activation=\"softmax\")) # Instanciando o modelo\n",
        "        modelMLP.compile( loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy']) # Instanciando o modelo\n",
        "\n",
        "        modelMLP.fit(X_trainK, y_trainK,callbacks = [checkpointer], epochs=100, batch_size=516,validation_data=(X_valK,y_valK),verbose=False) \n",
        "\n",
        "        ################ GWO ############################\n",
        "\n",
        "        grey = gwo(sizepop,fitnessGWO,-1,1,dimpop,ite,X_trainK,y_trainK,X_valK,y_valK,init=inicialpopulation.copy(),)# Rodando o GWO com os conjuntos de validação e treinamento e com o númer\n",
        "\n",
        "        modelGWO = loadmodelGWO() # Carregando o melhor modelo gerado pelo GWO para teste\n",
        "\n",
        "        testGWO.append(modelGWO.evaluate(X_test,y_test, verbose = False)[1]) # Avaliando o melhor modelo do GWO com o conjunto de teste\n",
        "        modelMLP.load_weights('model.weights.best.hdf5')\n",
        "        testMLP.append(modelMLP.evaluate(X_test,y_test, verbose = False)[1])\n",
        "        arrayfitnessGWO = []\n",
        "        \n",
        "        print(testMLP)\n",
        "        print(testGWO)\n",
        "\n",
        "\n",
        "    dataset, labels = shuffle(dataset, labels, random_state=cont) # Depois que termina as 10 iterações de um Kfold dou shuffle\n",
        "    cont += 1 # Valor da seed do shufle sendo somado +1\n",
        "\n",
        "datacsv = []\n",
        "\n",
        "for ite2 in range(iteKfold*kvalue):\n",
        "    aux = []\n",
        "    aux.append(testMLP[ite2])\n",
        "    aux.append(testGWO[ite2])\n",
        "    datacsv.append(aux)    \n",
        "\n",
        "\n",
        "datacsv = pd.DataFrame(datacsv, columns=['MLP','GWO'])\n",
        "\n",
        "dataplot = datacsv.boxplot(column=['MLP','GWO'],figsize=(12,8))\n",
        "plt.xlabel('Algorithms')\n",
        "plt.ylabel('Acuracy score')\n",
        "plt.title('Result')   \n",
        "\n",
        "print(\"media\")\n",
        "print(np.mean(testMLP))\n",
        "print(np.mean(testGWO))\n",
        "\n",
        "print(\"mediana\")\n",
        "print(np.median(testMLP))\n",
        "print(np.median(testGWO))\n",
        "\n",
        "print(\"std\")\n",
        "print(np.std(testMLP))\n",
        "print(np.std(testGWO))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}